import asyncio
import aiohttp
from dotenv import load_dotenv
from openai import OpenAI
import os
import sys
from .pdf_reader import extract_text_from_pdf
from .text_processor import (
    remove_newlines_and_carriage_returns,
    extract_from_introduction,
    remove_references,
    split_text_into_chunks,
    split_into_paragraphs
)

# NOTE: Much of this code was generated by either ChatGPT 4 or Claude 3 Opus and then further refined by myself. 
#       I am crediting the use of these tools per the instructions here: https://canvas.oregonstate.edu/courses/1958108/pages/exploration-can-and-should-you-use-ai-tools-in-capstone-2?module_item_id=24265452

load_dotenv()

CLIENT = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))  # Get the API key from the .env file
# ARTICLE_PATH = './test_pdfs/covid_article.pdf' # This is the path to the PDF file to be summarized, it will need to point to temporary cloud storage path when deployed.
CHUNK_SIZE = 4000  # This determines the size (in characters) of each chunk of text to be summarized
SUMMARY_LENGTH = 8000  # This determines the desired length (in characters) of the final summary
                       # We could give the user options for (short, medium, long) summaries and change this value dynamically.

async def fetch_summary(session, chunk, summary_length, num_chunks):
    """
    Fetches a summary of the given chunk using the OpenAI API.

    Args:
        session (aiohttp.ClientSession): The client session to make the API request.
        chunk (str): The text chunk to be summarized.
        summary_length (int): The desired length of the final summary.
        num_chunks (int): The total number of chunks.

    Returns:
        str: The summary of the chunk.
    """
    max_tokens = summary_length // num_chunks   # This ensures each chunk gets a proportional summary regardless of the length of the original text.
    async with session.post('https://api.openai.com/v1/chat/completions', json={
        "model": "gpt-3.5-turbo",
        "messages": [
            {"role": "system", "content": f"Summarize this: {chunk}"}  # TODO: See if there's a more meaningful way to prompt the AI to summarize the text.
        ],
        "max_tokens": max_tokens,            # I realized that instead of saying "Summarize this in 'n' sentences", we can just pass a max_tokens parameter.
        # temperature = 0.7,                 # We can also modify these parameters to control how the OpenAI API generates the output
        # top_p = 0.9,
        # frequency_penalty = 0.5,
        # presence_penalty = 0.1,
        # stop = ["\n"]
    }, headers={'Authorization': f'Bearer {CLIENT.api_key}'}) as response:
        result = await response.json()
        print(result)
        print('\n')
        return result['choices'][0]['message']['content']


async def summarize_chunks(chunks, summary_length):
    """
    Summarizes the given chunks using the OpenAI API.

    Args:
        chunks (list): A list of text chunks to be summarized.
        summary_length (int): The desired length (in characters) of the final summary.

    Returns:
        list: A list of summaries for each chunk.
    """
    async with aiohttp.ClientSession() as session:
        num_chunks = len(chunks)
        tasks = [fetch_summary(session, chunk, summary_length, num_chunks) for chunk in chunks]
        summaries = await asyncio.gather(*tasks)
    return summaries


async def summarize_file(pdf_path):
    """
    Summarizes the content of a PDF file.

    Args:
        pdf_path (str): The path to the PDF file to be summarized.

    Returns:
        str: The final summary of the PDF content.
    """
    text = extract_text_from_pdf(pdf_path)
    cleaned_text = remove_newlines_and_carriage_returns(text)
    text_from_introduction = extract_from_introduction(cleaned_text)
    final_text = remove_references(text_from_introduction)

    chunks = split_text_into_chunks(final_text, CHUNK_SIZE)
    summaries = await summarize_chunks(chunks, SUMMARY_LENGTH)

    joined_summary = ' '.join(summaries)
    final_summary = split_into_paragraphs(joined_summary, 9)
    # final_summary = format_summary(split_summary) this is pointless, formatting is lost when serialized and sent back to the frontend

    return final_summary


async def main(pdf_path):
    summary = await summarize_file(pdf_path)
    print(summary)

if __name__ == "__main__":
    pdf_path = sys.argv[1]  # Get the PDF path from command line arguments, this doesn't work
    asyncio.run(main(pdf_path))
    
import asyncio
import aiohttp
from dotenv import load_dotenv
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from openai import OpenAI
import os
import sys
from .pdf_reader import extract_text_from_pdf
from .text_processor import (
    remove_newlines_and_carriage_returns,
    extract_from_introduction,
    remove_references,
    split_text_into_chunks,
    split_into_paragraphs
)

# NOTE: Much of this code was generated by either ChatGPT 4 or Claude 3 Opus and then further refined by myself. 
#       I am crediting the use of these tools per the instructions here: https://canvas.oregonstate.edu/courses/1958108/pages/exploration-can-and-should-you-use-ai-tools-in-capstone-2?module_item_id=24265452

load_dotenv()
nltk.download('punkt')

CLIENT = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))  # Needs to be set in .env file

def estimate_complexity(text):
    """
    Estimates the complexity of a given text based on average sentence length and lexical diversity.

    Args:
        text (str): The input text to be analyzed.

    Returns:
        int: A recommended token count based on the estimated complexity of the text.
    """
    # Tokenize the text into sentences and words
    sentences = sent_tokenize(text)
    words = word_tokenize(text)
    num_sentences = len(sentences)
    num_words = len(words)

    # Calculate average sentence length
    avg_sentence_length = num_words / num_sentences if num_sentences > 0 else 0

    # lexical diversity indicates the range or "variety" of vocabulary that appears in a text segment
    unique_words = set(words)
    lexical_diversity = len(unique_words) / num_words if num_words > 0 else 0

    # Define complexity thresholds
    if avg_sentence_length > 20 and lexical_diversity > 0.5:
        return 500  # More tokens for high complexity
    elif avg_sentence_length > 15 or lexical_diversity > 0.3:
        return 300  # Medium complexity
    else:
        return 100  # Fewer tokens for simpler text


async def fetch_summary(session, chunk, num_chunks):
    """
    Fetches a summary of the given chunk using the OpenAI API.

    Args:
        session (aiohttp.ClientSession): The client session to make the API request.
        chunk (str): The text chunk to be summarized.
        num_chunks (int): The total number of chunks.

    Returns:
        str: The summary of the chunk.
    """
    max_tokens = estimate_complexity(chunk)  # Dynamically determine tokens based on complexity of the text
    async with session.post('https://api.openai.com/v1/chat/completions', json={
        "model": "gpt-3.5-turbo",
        "messages": [
            {"role": "system", "content": f"Summarize this: {chunk}"}
        ],
        "max_tokens": max_tokens,            
        # temperature = 0.7,               
        # top_p = 0.9,
        # frequency_penalty = 0.5,
        # presence_penalty = 0.1,
        # stop = ["\n"]
    }, headers={'Authorization': f'Bearer {CLIENT.api_key}'}) as response:
        result = await response.json()
        print(result)
        print('\n')
        return result['choices'][0]['message']['content']


async def summarize_chunks(chunks):
    """
    Summarizes the given chunks using the OpenAI API.

    Args:
        chunks (list): A list of text chunks to be summarized.

    Returns:
        list: A list of summaries for each chunk.
    """
    async with aiohttp.ClientSession() as session:
        num_chunks = len(chunks)
        tasks = [fetch_summary(session, chunk, num_chunks) for chunk in chunks]
        summaries = await asyncio.gather(*tasks)
    return summaries


async def summarize_file(pdf_path):
    """
    Summarizes the content of a PDF file.

    Args:
        pdf_path (str): The path to the PDF file to be summarized.

    Returns:
        str: The final summary of the PDF content.
    """
    text = extract_text_from_pdf(pdf_path)
    cleaned_text = remove_newlines_and_carriage_returns(text)
    text_from_introduction = extract_from_introduction(cleaned_text)
    final_text = remove_references(text_from_introduction)

    chunks = split_text_into_chunks(final_text)
    summaries = await summarize_chunks(chunks)

    joined_summary = ' '.join(summaries)
    final_summary = split_into_paragraphs(joined_summary, 9)

    return final_summary


# async def main(pdf_path):
#     summary = await summarize_file(pdf_path)
#     print(summary)

# if __name__ == "__main__":
#     pdf_path = sys.argv[1]  # Get the PDF path from command line arguments, this doesn't work
#     asyncio.run(main(pdf_path))
    